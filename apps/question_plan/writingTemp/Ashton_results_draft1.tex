% This will not build with the tomcat repo as it stands. 
% Draft only. 17 June 2022.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Author:             Salena T. Ashton
% Created:           17 June 2022

     
% Subject: Question-Asking and Plan Inference through Theory of Mind

% Key Topics: Question-Asking, Artificial Intelligence, Theory of Mind, Knowledge Representation,
                 %   Indirect Speech Acts, Psycholinguistics
                 
                
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\documentclass[10pt]{article}
\usepackage{geometry}  % Lots of layout options.  See http://en.wikibooks.org/wiki/LaTeX/Page_Layout
\geometry{letterpaper}  % ... or a4paper or a5paper or ... 
\usepackage{fullpage}  % somewhat standardized smaller margins (around an inch)
\usepackage{setspace}  % control line spacing in latex documents
\usepackage[parfill]{parskip}  % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amsmath,amssymb}  % latex math
\usepackage{empheq} % http://www.ctan.org/pkg/empheq
\usepackage{bm,upgreek}  % allows you to write bold greek letters (upper & lower case)
\usepackage{url}
\usepackage{comment}
\usepackage{fancyhdr}
% for typsetting algorithm pseudocode see http://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode
\usepackage{algorithmic,algorithm}  
\usepackage{graphicx}  % inclusion of graphics; see: http://en.wikibooks.org/wiki/LaTeX/Importing_Graphics
% allow easy inclusion of .tif, .png graphics
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{xspace}
\newcommand{\latex}{\LaTeX\xspace}
\usepackage{color}  % http://en.wikibooks.org/wiki/LaTeX/Colors
\long\def\ans#1{{\color{blue}{\em #1}}}
\long\def\ansnem#1{{\color{blue}#1}}
\long\def\boldred#1{{\color{red}{\bf #1}}}
% Useful package for syntax highlighting of specific code (such as python) -- see below
\usepackage{listings}  % http://en.wikibooks.org/wiki/LaTeX/Packages/Listings
\usepackage{textcomp}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{stackengine} % to overlap symbols


%%% The following lines set up using the listings package
\renewcommand{\lstlistlistingname}{Code Listings}
\renewcommand{\lstlistingname}{Code Listing}
%%% Specific for python listings
\definecolor{gray}{gray}{0.5}
\definecolor{green}{rgb}{0,0.5,0}
\lstnewenvironment{python}[1][]{
\lstset{
language=python,
basicstyle=\footnotesize,  % could also use this -- a little larger \ttfamily\small\setstretch{1},
stringstyle=\color{red},
showstringspaces=false,
alsoletter={1234567890},
otherkeywords={\ , \}, \{},
keywordstyle=\color{blue},
emph={access,and,break,class,continue,def,del,elif ,else,%
except,exec,finally,for,from,global,if,import,in,i s,%
lambda,not,or,pass,print,raise,return,try,while},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{green},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
upquote=true,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray}\slshape,
emph={[4]1, 2, 3, 4, 5, 6, 7, 8, 9, 0},
emphstyle=[4]\color{blue},
literate=*{:}{{\textcolor{blue}:}}{1}%
{=}{{\textcolor{blue}=}}{1}%
{-}{{\textcolor{blue}-}}{1}%
{+}{{\textcolor{blue}+}}{1}%
{*}{{\textcolor{blue}*}}{1}%
{!}{{\textcolor{blue}!}}{1}%
{(}{{\textcolor{blue}(}}{1}%
{)}{{\textcolor{blue})}}{1}%
{[}{{\textcolor{blue}[}}{1}%
{]}{{\textcolor{blue}]}}{1}%
{<}{{\textcolor{blue}<}}{1}%
{>}{{\textcolor{blue}>}}{1},%
%framexleftmargin=1mm, framextopmargin=1mm, frame=shadowbox, rulesepcolor=\color{blue},#1
framexleftmargin=1mm, framextopmargin=1mm, frame=single,#1
}}{}
%%% End python code listing definitions

%%% Specific for matlab listings
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 
\lstnewenvironment{matlab}[1][]{
\lstset{ %
  language=Matlab,                % the language of the code
  basicstyle=\footnotesize,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=2,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=t,                   % sets the caption-position to top
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*)},            % if you want to add LaTeX within your code
  morekeywords={*,...}               % if you want to add more keywords to the set
  framexleftmargin=1mm, framextopmargin=1mm, frame=single,#1 % display caption
} }{}
%%% End matlab code listing definitions

%% hilite
\newcommand{\hilite}[1]{\colorbox{yellow}{#1}}
\long\def\todo#1{\hilite{{\bf TODO:} {\em #1}}}


% ----- ----- Probability Template I made for math 564 ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- 
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{changepage}
\usepackage{graphicx}
\usepackage{tikz} 
\usepackage{multicol} 

\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
%\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\bib}{\color{black \citep}}

% See https://latexdraw.com/how-to-draw-venn-diagrams-in-latex/
       % for venn diagram tutorials
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{hyperref}
\hypersetup{colorlinks=true,allcolors=[rgb]{0,0, 1}}
\hypersetup{urlcolor=cyan}

%This package needs double curly brackets to retain title capitalization.
\usepackage{natbib}

% I altered the plainnat.bst text to have Turabian Flavor of place last name first, first name last. 
% See line 222 of file to change back.
\bibliographystyle{plainnat}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%     Begin Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This section is from the repo that Adarsh used for stub
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% uncomment these two before submitting
%\chapter{Question-Asking and Plan Inference}
%\label{ch:question_plan}


%%% delete this two before submitting
\textbf{Question-Asking and Plan Inference}


\textbf{Salena Ashton, Loren Rieffer-Champlin, Liang Zhang,
Adarsh Pyarelal, Clayton Morrison}

\section{Introduction}

In the SAR scenario for ASIST's Minecraft virtual environment, teams of human players engage in cooperative behavior to search for, stabilize, and rescue victims within a collapsed building. As human players verbalize plans, make suggestions, or tell each other what to do, they also ask questions that can infer hidden goals or intentions. Teammates reduce their individual knowledge asymmetry by asking and answering questions. Using Theory of Mind (ToM), \footnote{The capacity to infer another's thoughts, feelings, beliefs or intentions.}, we will investigate how uttered questions can infer another person's goal or intention. 

This investigation will guide future research into knowledge engineering and representation of tasks, goals, and hidden intentions. While simple human goals may be represented with classical AI planning, complex goals that have constraints or multiple levels of abstraction may be best represented by a \emph{hierarchical task network} (HTN), which is a tree of possible plans.\footnote{Technically, the plans
    produced by HTN planners can also be represented with flat lists - however,
in this section, we use the term `plan' to refer to the actual `plan tree' that
contains the task decompositions as well, rather than just the plan alone.}




We will investigate the following research questions for Study-3:

\begin{enumerate}
    \item How do spoken questions reveal another person’s plan or intent? 
    \item Can people listen to spoken questions and accurately decide if the other person's plan
        is simple, sequential, or hierarchical? Can the distinction between such
        structures improve predictive performance for Artificial Social
        Intelligence (ASI) agent intervention?
\end{enumerate}

%%%%%
Previous research into question-\emph{answering, not asking,} centered on optimization because researchers assumed that people prefer concise questions and answers. However, people do not engage in dialog using question or answer sets. They may ask open-ended questions, meander in speech without purpose, and use indirect speech acts to express mutual goals or build rapport with each other. The current literature regarding ToM and question-answering is sparse but even more so for question \emph{asking}. 

Hawkins and Goodman connect question-asking and intention \emph{because} of the scarcity of “empirical evidence about how social context affects the questioner’s choice." They redefined the meaning of a question as "the interpretation and response of a hidden (or uttered) goal, to be discerned by another person, typically a dialog partner" \citep{hawkins_goodman_2017}. Hawkins and Goodman describe speech acts and question-answer dialog as a form of \emph{information asymmetry}: A questioner has a goal but needs information while an answerer has information but does not know the questioner’s goal. The type of questions then asked will depend on context, social inference, and signals in a dialog setting. The significance of their work is in the decoupling of the inferred goal from the explicit meaning of the question to model context and avoid assumptions. \footnote{Their work was limited to epistemic questions and cooperative behavior.}

Deciphering a person’s goal or intention from their answer to a question, instead of the question itself, may be another way to understand intent. While Hawkins and Goodman define questions as hidden goals or intentions, Mehdi Alaima et al define the act of asking questions as ‘the providing of information or knowledge to reinforce knowledge one way or another," independently paralleling the definition given by Hawkins and Goodman. "When information is missing, or contradicts what one knows, a knowledge goal will arise, often leading to the generation of questions. The person is then made aware of the information needs, and motivated to formulate a question to obtain the missing knowledge,” \citep{alaimi_2020}. Building on the claims of Hawkins and Goodman, and Alaima et al, we investigate question-asking within the SAR scenario of ASIST’s Minecraft environment. 

\section{Approach}

We assume that questions have hidden goals and infer plans. As teams ask more
questions of each other, human team ToM converges toward cooperative behavior.
We will investigate whether question-asking is associated with \emph{team
planning}, defined as a set of goals, strategies, or tasks that are executed.
We define \emph{coordination} as behaviors and utterances to create a common
plan or strategy\footnote{Note that this is distinct from the mathematical
definition of coordination proposed in \autoref{ch:pgm}}. We define
\emph{cooperation} as team behaviors that implement an already-agreed up on
plan.

To capture hidden goals, inferred plans, and patterns that may represent human
ToM, we will annotate six ASIST Study 3 Spiral 2 pilot video observations and
six HSR ASIST Study 3 videos released between March 29 and May 5, for a total
of at least twelve videos. These videos are of three distinct missions for each
team. Due to the expensive costs of taxonomy label development with strict
adherence to grounded theory methodology, this stage of the experiment is
limited to no less than twelve videos. 

Two human annotators will code all uttered questions between teammates within the Minecraft SAR scenario.
We use the qualitative coding procedure known as Grounded Theory,
as defined by \citet{corbin_strauss_2015}. 

More specifically, and as defined by \citet{saldana_2021}, we will use a
Grounded Theory Process Coding for a state or action across some interval of
time. These \emph{grounded-in-data} labels are known as \emph{concept-level}
labels, which are the smallest pieces of data that encode a question-asking
phenomena of interest. We also use this coding methodology to investigate the connectivity and
causality of each concept label to discover possible relationships between
presence or absence of team actions, interactions, conditions, and consequences
of question-asking. Densely-connected concept labels suggest subcategories and
categories. Sparsely-connected labels will not be discarded; they will be used
to consider variability within patterns and categories that emerge. In cases
where questions have co-reference or other contextual dependencies, only that
direct dependency will be coded for local semantic meaning.

We make the following considerations when creating codes: 

\begin{itemize}
    \item Frequency will not dictate importance, causality, or connectivity of a concept
    \item Each question will have at least one annotation and up to four
      annotations:
    \begin{itemize}
        \item Primitive actions (ground truth). Ex: breakRubble,
          requestStabilizedVictimCarry.
        \item Abstraction Levels of actions (of primitive actions) Respective
          examples: respondRubbleRequest or createVictimAccess, collaborateStabilizedVictim
    \end{itemize}
    \item Labels will be stemmed and minimally normalized
    \item Capturing the phenomena of question-asking across time, between any subset of a team, between the same team across the two different missions. 
\end{itemize}


To avoid annotator and researcher biases and any \emph{a priori} belief on
which team ToM strategies may be used, concept and category labels are not
pre-determined. Inter-annotator agreement must reach a Kappa Score of 80\% or
higher. This also gives a more solid, grounded analytical meaning to any
emergent categories. 

After the development of labels and taxonomy, the investigation of team ToM and
question-asking will scale for additional videos. When all concepts,
subcategories, categories can reasonably explain the phenomena of the video
observations, one or two super-categories, \emph{theories of team plan}, will
emerge. We currently assume that a theory of team plan would have greater
predictive power and ToM inference potential. 


\section{Evaluation}

Because of the small sample size of this investigation, we will not perform a
quantitative evaluation at this time. Instead, we will perform a qualitative
investigation of word frequencies, clustering patterns, and correlation of
annotator-generated labels through data visualization. Below is a list of
possible visualizations we may consider:

\begin{itemize}

    \item Connectivity of concept-level labels: radial diagrams, arc diagrams,
        matrix diagrams or graph networks

    \item Frequency patterns of words or concept labels (normalized, word count
        / total number of words in that question): scatterplots or histograms

    \item Correlation of words and labels with time: time series, scatterplots. 

    \item Concept-level subcategorization(s): clustering, PCA (concept labels
        possibly projected onto sub-categorical spaces), or hierarchical
        visualizations.

\end{itemize}


These visualizations serve as a preliminary analysis of how uttered
questions, with their hidden goals, could map human ToM to AI planning data
structures. The following visualization shows the distribution of
questions asked across time for two pilot study teams. 

\begin{comment}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{../images/prelim_percent_questions.pdf}
    \caption{Questions asked during three missions of two distinct teams across
    time. Black indicates questions asked toward the beginning of each mission
  and red indicates questions asked toward the end of each mission. This 
suggests that teams ask more questions toward the beginning of the
first mission and the end of the last mission.}
\end{figure}
\end{comment}

Such visualizations, based on twelve videos, will lead to further insight
through this investigation. Future measures may include Mann-Whitney U-Tests,
t-tests (only if we annotate a large-enough sample), precision and recall of
the concept-level and category patterns to describe the generalizability for
real data with no ASI interventions, generalizability for real data with ASI
interventions, and the variance of patterns in label categories. Another
possible measure, for future research, would be the F1 score to explain how
well these labels describe observations without ASI interventions, when
compared to high-intervention observations. This future investigation would
address measure ASI-M5: Coordinative Communications to
measure teamwork, include additional video observations for real data in Study
3, and continue our investigation of whether human plans and ToM are best
represented by classical planning or HTN planning.

\newpage







\section{Results}












PROMISES MADE:

\begin{itemize}
    \item DONE: investigate how uttered questions can infer another person's goal or intention. 
    

    \item How do spoken questions reveal another person’s plan or intent? 
    \item Can people listen to spoken questions and accurately decide if the other person's plan
        is simple, sequential, or hierarchical? Can the distinction between such
        structures improve predictive performance for Artificial Social
        Intelligence (ASI) agent intervention?
    \item  annotate six ASIST Study 3 Spiral 2 pilot video observations and
six HSR ASIST Study 3 videos released between March 29 and May 5, for a total
of at least twelve videos. 
    \item Two human annotators
    \item Strict adherence to Grounded Theory (bottomup/ process and causal coding)
    \item emergent categories
    \item minimally-stemmed or lemmatized, 100\% unsupervised labeling with full autonomy and strict definition/ use
    \item Kappa $>$ 80\%
    \item 1 - 2 super theories will emerge, which we assume to mean Team ToM
    \item Preliminary Analysis HOW Team TOM maps to AI PLanning-- due to small sample size:
    \begin{itemize}
        \item qualitative investigation of word frequencies, clustering and/ correlation
        \item qualitative investigation through visualization
        \item scatterplots or histograms or hierarchical visualizations
        \item information across time
    \end{itemize}
 \end{itemize}

Limitations:
\begin{itemize}
    \item Cost of human annotation
    \item sample size means no evaluation at this time
\end{itemize}

Future Research:
\begin{itemize}
    \item knowledge engineering and representation of tasks, goals, and hidden intentions;  \emph{hierarchical task network} (HTN),
    \item scale for additional data
    \item Preliminary research can lead to additional annotation, quantitative analysis and generalized theories for ASI
    \item Compare and contrast theories that emerge from no-intervention data to high-intervention data
    \item Investigation into which data structures best capture human individual and team ToM
\end{itemize}








\vspace{300pt}
\subsection{Label Development and Taxonomy}

To demonstrate that questions have hidden goals or intentions, we used a bottom-up approach of Grounded Theory Qualitative Coding methodology. This particular method is one of the strictest and most robust qualitative methodologies that easily transforms into quantitative analysis, and we followed its canon and procedure with care. Two independent annotators\footnote{Ashton and Kim} viewed six different study-3 pilot videos using an unconstrained set of labels. We annotated one video and compared notes to address the others' potential bias toward labels and their lexical definitions. The disambiguation of such labels led to extensive documentation of verb, noun and modifier use agreement. 

These labels served to develop a final labeling schema for the remaining 5 pilot videos and the annotation of six different study-3 real data videos\footnote{Ashton and Reiffer-Champlin}. The following features were annotated: video observation number, question count per video, running count of regular and critical victims fully rescued at time interval of occurrence, running score, time interval of question utterance, the player who asked the question, transcription of question utterance\footnote{Disclaimer: Ashton wears corrective hearing aids; all annotation transcriptions were verified by both annotators in stage 1 and stage 2}), optional context of question utterance, and the question label. 

For all 12 video observations, Ashton further annotated a causeLabel, effectLabel, abstractLabel, htnLabel, and reasonWhyQuestionLabel before seeing any of the other annotators' work. A cause is the reason why a question had been uttered or the capturing of an event or phenomena that made the question utterance possible at that time step whereas it had not been a plausible question before. While there may be several causes for a question utterance, we would need more data to confidently declare whether these causeLabels are independent or conditionally independent. The same can be said for several effectLabels resulting from question utterances. Abstract labels redefine the question label at a mid-range abstraction level and the htn level redefines the entire cause-question-effect triple as a highest-level task or goal. 

While annotation label schema had been developed, no constraint was placed on any potential label or event; each annotator remained autonomous in labeling any question utterance. Because only three additional labels were created during this second stage of coding, we achieved \emph{theoretical saturation}.\footnote{The point of label development that allows for additional labels, but very few are created when given full autonomy, thus indicating that existing label schema fully capture events, data and phenomena observed.} A full, lexical definition of labels can be found within this repository that includes examples of when or when not to use a particular label. Label codes were created in the format of "verbObjectObject" where an object could be a noun or modifier, and was left for the annotator to decide.  


Data preprocessing on pilot data and adherence to Grounded Theory procedure suggested that the theoretical saturation point of concept labels  yield 21 verb labels, 24 noun labels, and 25 modifier labels. Without replacement, this yields 12,600 possible labels, plus any label creations given to the autonomous annotators. Regardless of the probability of these labels approaching near zero, Ashton demonstrated theoretical saturation with the demonstrated use of less than 30 labels, before global replacement.

The strict adherence to Grounded Theory worked, as described by Saldana, but it now lets the data tell us what the player intention and strategies are.



\subsection{Data Cleaning and Annotator Agreement}
Because of the full autonomy given annotators, the probability of theoretical label agreement approaches zero, so it is remarkable that we achieved an unweighted Cohen Kappa agreement of 0.892. Data were entered in an Excel spreadsheet to minimize annotator interface constraints.\footnote{Future annotations will use particular software to allow for experiment scalability} and cleaned by spelling correction, minimal lemmatization, and alphabetizing one object after the other, both placed after the verb. During data analysis, Ashton empirically justified granularities of label definition to capture question-asking intentions. For example, the conditional probabilities of questions, given a player intention would change if "critical" and "victim" were labeled as "victim". Therefore, these and similar granularities were maintained. Other conditional probabilities, such as "location" and "room" did not significantly change, so after the data were cleaned, a global replacement of "room" to "location", and similar replacements were made. Global placement after post processing is not reflected in the Kappa score in order to maintain researcher transparency.

Using the disambiguation documentation and Merriam-Webster's Dictionary to meticulously settle any annotator label disagreement, Ashton declared the question labels as 'agree' or 'disagree'. In cases of agreement but labels were not the same, "clarifyBreakRubble" vs "confirmBreakRubble" for example, Ashton verified annotator intention. Likewise, in cases of disagreement but labels sounded similar, "navigateRoom" vs "navigateLocation", Ashton declared 'disagreement'\footnote{Clarify means to ask for additional information; confirm is to give additional acknowledgement of additional information; Location was used as a general area or set of rooms in the same area; Room was used as a specific room only.} When annotator intention and the disambiguation documentation did not clarify the agreement or disagreement of labels, Ashton declared it as 'disagreement'. In cases where question labels had matching cause or effect labels, Ashton declared the question label as 'agreement'. In cases where questionLabels had ambivalent annotator intention, cause or effect label pairs, Ashton then viewed the abstraction label. In cases of 3 or less ambivalent label subsets, Ashton declared "disagreement; cannot justify agreement with data size." All decisions are thoroughly documented in the dataset under the column 'reasonWhyQuestionLabel.' 





I defined 'agreement' as $0 = $ No match and $1 =$ match\footnote{Explanation of a match is found in the data themselves for each observation}. This binary classification of agreement allowed me to use the unweighted Cohen Kappa\footnote{In order to adhere to the robust procedure of Grounded Theory, the technical calculation of Kappa's score would include the probability of \textit{all possible probabilities of all labels}, which would approach zero. While it sounds trivial, it must be mentioned that the reason Grounded Theory is robust in its unsupervised labeling approach is because annotators have complete autonomy. So, while it is mathematically true that the probability of two annotators choosing the same label for any action assumes infinite label possibilities, I chose to calculate the probability with the assumption of 21 verb possibilities and 25 noun/ modifier possibilities, with the added component of user-created labels. If theoretical saturation were to be achieved, very few additional labels would be created. That was the case here. Therefore, the Kappa score was calculated on the latter set of assumptions. Inter-rater Reliability measure. Because the sample size is small and only two annotators worked on the pilot data, and two annotators worked on the real data, there is no current justification for using Scott's pi or a weighted Kappa. This will change as investigation and further research scale.}


This rigorous approach to label disambiguation fulfilled several purposes: annotator assumption about domain definitions for SAR, annotator bias toward predicted strategy of player or team, and explicit statement of reasoning that will guide algorithmic development of causal inference in future research. We promised a Kappa Score of 80\% inter-rater reliability and we deliver a score of 89.2\%. 


\subsection{Emergent Categories of Player Intention}
From more than 400 unconstrained labels developed in stage 1 annotation, we discovered recurrent components from which goals or intentions emerged:

\begin{itemize}
    \item Talking \emph{at or to} a teammate and not \emph{with} a teammate: direct, suggest, etc.
    \item Talking \emph{with} a teammate: ask, request, answer, clarify, etc.
    \item Intention toward position: location, navigate, destination, room, etc.
    \item Prioritizing an idea: plan, suggest, request, collaborate, critical, victim, etc.
    \item Question utterances that were actually demands, statements that were questions with inflection, and other nuanced utterances: suggest, tell, direct, request; context and explanations included in annotation.
    \item Autonomy in any other label creation; this resulted in marker, mission, wake, etc.
\end{itemize}



\subsubsection{Questions and Intentions}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/abstractLabel_ConditionalProbability_STA.pdf}
    \caption{ }
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/abstractLabelProbability_STA.pdf}
    \caption{ }
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/questionLabel_ConditionalProbability_STA.pdf}
    \caption{ }
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/questionLabel_ConditionalProbability_STA.pdf}
    \caption{ }
\end{figure}


\clearpage

\subsubsection{Visualizations, Frequencies and Correlations}

We did not regard label frequency as criteria for label usefulness. Instead, from these emergent categories, we chose specific words that optimized intention or belief differences among player interaction\footnote{For example: \emph{ask}, \emph{request}, \emph{suggest}, and \emph{clarify} are all actions that show the goal of obtaining information, but clarify asks for additional information needed before acting upon a goal or task, request is an initiation of commitment to another player with the optional accept/ reject response that suggest would not imply, and ask is a general desire for information.}. We then used 38 label components, to be used as verb or object, as per the discretion of each annotator, and in conjunction with annotator autonomy for new label development, for stage 2 annotation. After data cleaning, a handful of labels were created but most were subsumed in lexical meaning of established labels. Only 3 additional labels were remained: search, plan, and role, and are found in the data.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/QuestionTiming_STA.pdf}
    \caption{ }
\end{figure}

\clearpage












\subsubsection{Relations and Causality, Categories, and Theories}

\subsection{Preliminary Analysis and Continued Research}
The ultimate goal of this work is two-fold: find enough evidence to warrant the continued investigation of question-asking through causal reasoning to discern player intention and to algorithmitize player intention from player utterance transcriptions of player without human annotator dependency. 

Due to the small sample size, we do not offer conclusive evaluations at this time. However, from these 12 video annotations, we \emph{can warrant} the further investigation of question-asking and inferred goals or intentions among human team players.

Key question phrases to give clues to ASI agents:
\begin{itemize}
    \item "If"
    \item
\end{itemize}



\textbf{TODO: write about scalability, additional observations, super categories, theories, no evaluations at this time, word frequencies and probabilities, visuals, clustering and CPCA, current thoughts on mapping question utterances to goals to human ToM, then be sure to link code to repository and link this paper to the code and repository after i push to main If I need more than one page for visuals, I can add an appendix to this document that is on tomcat... make sure that I use the section part for the main.tex}


% End of stub Adarsh used for Repo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

% To avoid spacious justification of the bibliography:
\raggedright
\bibliography{bibliography} % Name of bib file

\end{document}





